html_nodes('.board_list') %>%
html_nodes('li') -> lis
lis
html %>%
html_nodes('.box_detail_content') %>%
html_nodes('.board_list') %>%
html_nodes('li') -> lis
html %>%
html_nodes('.content_left') %>%
html_nodes('.board_list') %>%
html_nodes('li') -> lis
lis
html %>%
html_nodes('content_middle') %>%
html_nodes('.board_list') %>%
html_nodes('li') -> lis
lis
url_base <- 'https://movie.naver.com/movie/bi/mi/point.nhn?code=163788#tab'
url <- paste0(url_base, encoding="euc-kr")
html <- read_html(url)
head(html)
html
html %>%
html_nodes('.obj_section') %>%
html_nodes('.score_result') %>%
html_nodes('li') -> lis
lis
view(lis)
View(lis)
html %>%
html_nodes('.content_middle') %>%
html_nodes('.board_list') %>%
html_nodes('li') -> lis
lis
url_base <- 'http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&mallGb=KOR&barcode=9788956604992&orderClick=LAG&Kc=#N'
html <- read_html(url_base,encoding = "EUC-kr")
url_base <- 'http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&mallGb=KOR&barcode=9788956604992&orderClick=LAG&Kc=#N'
html <- read_html(url_base,encoding = "EUC-kr")
html %>%
html_nodes('.content_middle') %>%
html_nodes('.board_list') %>%
html_nodes('li') -> lis
lis
url_base <- 'http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&mallGb=KOR&barcode=9788956604992&orderClick=LAG&Kc=#N'
html <- read_html(url_base,encoding = "EUC-kr")
html %>%
html_nodes('.board_list') %>%
html_nodes('.id') %>%
html_nodes('kloverRating') %>%
html_nodes('txt') %>%
html_nodes('li') -> lis
library(rvest)
library(stringr)
library(dplyr)
library(httr)
trim <- function(x) gsub("^\\s+|\\s+$", "", x)
url_base <- 'http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&mallGb=KOR&barcode=9788956604992&orderClick=LAG&Kc=#N'
html <- read_html(url_base,encoding = "EUC-kr")
html %>%
html_nodes('.board_list') %>%
html_nodes('.id') %>%
html_nodes('kloverRating') %>%
html_nodes('txt') %>%
html_nodes('li') -> lis
lis
url_base <- 'https://movie.naver.com/movie/bi/mi/point.nhn?code=163788#tab'
url <- paste0(url_base, encoding="euc-kr")
html <- read_html(url)
head(html)
html
score <- c()
reple <- c()
company <- c()
name <- c()
for (li in lis) {
star_score <- html_node(li, '.star_score')
score <- c(score, trim(html_text(star_score, 'em')))
li %>%
html_node('.score_reple') %>%
html_text('.score_reple') %>%
trim() %>%
str_split("\r\n") %>%
.[[1]] -> rep
reple <- c(reple, trim(rep[1]))
company <- c(company, trim(rep[2]))
name <- c(name, str_sub(trim(rep[4]), 3, -1))
}
html %>%
html_nodes('.obj_section') %>%
html_nodes('.score_result') %>%
html_nodes('li') -> lis
lis
score <- c()
reple <- c()
company <- c()
name <- c()
for (li in lis) {
star_score <- html_node(li, '.star_score')
score <- c(score, trim(html_text(star_score, 'em')))
li %>%
html_node('.score_reple') %>%
html_text('.score_reple') %>%
trim() %>%
str_split("\r\n") %>%
.[[1]] -> rep
reple <- c(reple, trim(rep[1]))
company <- c(company, trim(rep[2]))
name <- c(name, str_sub(trim(rep[4]), 3, -1))
}
review = data.frame(score=score, reple=reple, company=company, name=name)
review
trim <- function(x) gsub("^\\s+|\\s+$", "", x)
url_base <- 'https://movie.naver.com/movie/bi/mi/point.nhn?code=163788#tab'
url <- paste0(url_base, encoding="euc-kr")
html <- read_html(url)
head(html)
html
html %>%
html_nodes('.obj_section') %>%
html_nodes('.score_result') %>%
html_nodes('li') -> lis
lis
score <- c()
reple <- c()
company <- c()
name <- c()
for (li in lis) {
star_score <- html_node(li, '.star_score')
score <- c(score, trim(html_text(star_score, 'em')))
li %>%
html_node('.score_reple') %>%
html_text('.score_reple') %>%
trim() %>%
str_split("\r\n") %>%
.[[1]] -> rep
reple <- c(reple, trim(rep[1]))
company <- c(company, trim(rep[2]))
name <- c(name, str_sub(trim(rep[4]), 3, -1))
}
review = data.frame(score=score, reple=reple, company=company, name=name)
review
html %>%
html_nodes('.box_detail_content') %>%
html_nodes('.borad_list') %>%
html_nodes('li') -> lis
lis
url_base <- 'http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&mallGb=KOR&barcode=9788956604992&orderClick=LAG&Kc=#N'
html <- read_html(url_base,encoding = "EUC-kr")
html %>%
html_nodes('.box_detail_content') %>%
html_nodes('.borad_list') %>%
html_nodes('li') -> lis
lis
html %>%
html_nodes('.box_detail_review') %>%
html_nodes('.borad_list') %>%
html_nodes('li') -> lis
lis
html %>%
html_nodes('.borad_list') %>%
html_nodes('li') -> lis
lis
html %>%
html_nodes('.borad_list')
url_base <- 'http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&mallGb=KOR&barcode=9788956604992&orderClick=LAG&Kc=#N'
html <- read_html(url_base,encoding = "EUC-kr")
html %>%
html_nodes('.borad_list')
url_base <- 'http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&mallGb=KOR&barcode=9788956604992&orderClick=LAG&Kc=#N'
html <- read_html(url_base,encoding = "EUC-kr")
html %>%
html_nodes('.borad_list')
View(html)
html_nodes(html, '.borad_list')
html <- read_html(url_base, encoding = "euc-kr")
html_nodes(html, '.borad_list')
html_nodes(html, '.box_detail_review')
html %>%
html_nodes('.box_detail_review')
html %>%
html_nodes('.box_detail_review') %>%
html_nodes('.board_list')
html %>%
html_nodes('.content_middle') %>%
html_nodes('.content_left') %>%
html_nodes('.box_detail_content') %>%
html_nodes('.box_detail_review') %>%
html_nodes('.board_list') %>%
lis
html %>%
html_nodes('.content_middle') %>%
html_nodes('.content_left') %>%
html_nodes('.box_detail_content') %>%
html_nodes('.box_detail_review') %>%
html_nodes('.board_list') %>%
html %>%
html_nodes('.content_middle') %>%
html_nodes('.content_left') %>%
html_nodes('.box_detail_content') %>%
html_nodes('.box_detail_review') %>%
html_nodes('.board_list') %>%
html_nodes('li') -> lis
url_base <- 'http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&mallGb=KOR&barcode=9788956604992&orderClick=LAG&Kc=#N'
html <- read_html(url_base, encoding = "euc-kr")
html %>%
html_nodes('.content_middle') %>%
html_nodes('.content_left') %>%
html_nodes('.box_detail_content') %>%
html_nodes('.box_detail_review') %>%
html_nodes('.board_list') %>%
html_nodes('li') -> lis
lis
url_base <- 'https://movie.naver.com/movie/bi/mi/basic.nhn?code=173123'
url <- paste0(url_base, encoding="euc-kr")
html <- read_html(url)
head(html)
html
html %>%
html_nodes('.obj_section') %>%
html_nodes('.score_result') %>%
html_nodes('li') -> lis
lis
html %>%
html_nodes('.input_netizen') %>%
html_nodes('.score_result') %>%
html_nodes('li') -> lis
lis
# NAVER 영화('스파이더맨') 전문가리뷰 크롤링
library(rvest)
library(stringr)
library(dplyr)
trim <- function(x) gsub("^\\s+|\\s+$", "", x)
# NAVER 영화('스파이더맨') 전문가리뷰 크롤링
library(rvest)
library(stringr)
library(dplyr)
trim <- function(x) gsub("^\\s+|\\s+$", "", x)
url_base <- 'https://movie.naver.com/movie/bi/mi/point.nhn?code=173123#pointAfterTab'
url <- paste0(url_base, encoding="euc-kr")
html <- read_html(url)
head(html)
html
url <- paste0(url_base, encoding="UTF-8")
html <- read_html(url)
head(html)
html
html %>%
html_nodes('.input_netizen') %>%
html_nodes('.score_result') %>%
html_nodes('li') -> lis
lis
url_base <- 'https://movie.naver.com/movie/bi/mi/point.nhn?code=173123#pointAfterTab'
url <- paste0(url_base, encoding="UTF-8")
html <- read_html(url)
head(html)
html
html %>%
html_nodes('.input_netizen') %>%
html_nodes('.score_result') %>%
html_nodes('li') -> lis
html %>%
html_nodes('.score_result') %>%
html_nodes('li') -> lis
lis
html %>%
html_nodes('.ifr_module2')
html_nodes('.score_result') %>%
html_nodes('li') -> lis
url <- paste0(url_base, encoding="euc-kr")
html <- read_html(url)
head(html)
html
html %>%
html_nodes('.ifr_module2')
html_nodes('.score_result') %>%
html_nodes('li') -> lis
html %>%
html_nodes('.ifr_module2') %>%
html_nodes('.score_result') %>%
html_nodes('li') -> lis
lis
txt <- readLines("ll.txt",encoding = "euc-kr")
setwd("D:/Workspace/R_Project")
txt <- readLines("ll.txt",encoding = "euc-kr")
ts <- extractNoun(txt)
library(tm)
library(SnowballC)
library(dplyr)
library(KoNLP)
library(wordcloud2)
useSejongDic()
txt <- readLines("ll.txt",encoding = "euc-kr")
ts <- extractNoun(txt)
View(ts)
ts
View(ts)
View(ts)
ts <- gsub("ㅋ", ts)
ts <- gsub("ㅋ","", ts)
View(ts)
# data <- read.csv("long_live.csv", encoding = "euc-kr")
#
# ds <- data %>%
#   select(txt_list)
#
# write.table(ds,"ll.txt", row.names = FALSE)
mergeUserDic(data.frame(c('오태식'),c('핵노잼'),c('노잼')))
# data <- read.csv("long_live.csv", encoding = "euc-kr")
#
# ds <- data %>%
#   select(txt_list)
#
# write.table(ds,"ll.txt", row.names = FALSE)
newUserDic(data.frame(c('오태식'),c('핵노잼'),c('노잼')))
# data <- read.csv("long_live.csv", encoding = "euc-kr")
#
# ds <- data %>%
#   select(txt_list)
#
# write.table(ds,"ll.txt", row.names = FALSE)
buidDictionary(data.frame(c('오태식'),c('핵노잼'),c('노잼')))
library(tm)
library(SnowballC)
library(dplyr)
library(KoNLP)
library(wordcloud2)
useSejongDic()
# data <- read.csv("long_live.csv", encoding = "euc-kr")
#
# ds <- data %>%
#   select(txt_list)
#
# write.table(ds,"ll.txt", row.names = FALSE)
buidDictionary(data.frame(c('오태식'),c('핵노잼'),c('노잼')))
# data <- read.csv("long_live.csv", encoding = "euc-kr")
#
# ds <- data %>%
#   select(txt_list)
#
# write.table(ds,"ll.txt", row.names = FALSE)
mergeUserDic(data.frame(c('오태식'),c('핵노잼'),c('노잼')))
# data <- read.csv("long_live.csv", encoding = "euc-kr")
#
# ds <- data %>%
#   select(txt_list)
#
# write.table(ds,"ll.txt", row.names = FALSE)
mergeUserDic(data.frame(c('오태식')))
# data <- read.csv("long_live.csv", encoding = "euc-kr")
#
# ds <- data %>%
#   select(txt_list)
#
# write.table(ds,"ll.txt", row.names = FALSE)
mergeUserDic(data.frame('오태식'))
# data <- read.csv("long_live.csv", encoding = "euc-kr")
#
# ds <- data %>%
#   select(txt_list)
#
# write.table(ds,"ll.txt", row.names = FALSE)
mergeUserDic(data.frame('오태식',"핵노잼","노잼잼"))
# data <- read.csv("long_live.csv", encoding = "euc-kr")
#
# ds <- data %>%
#   select(txt_list)
#
# write.table(ds,"ll.txt", row.names = FALSE)
buildDictionary(ext_dic = "woorimalsam", category_dic_nms = "", user_dic =
data.frame('오태식',"핵노잼","노잼"))
txt <- gsub("ㅋ","", txt)
txt <- gsub("ㅠㅠ","",txt)
txt <- readLines("ll.txt")
library(tm)
library(SnowballC)
library(dplyr)
library(KoNLP)
library(wordcloud2)
useSejongDic()
txt <- readLines("ll.txt")
buildDictionary(ext_dic = "woorimalsam", category_dic_nms = "", user_dic =
data.frame('오태식',"핵노잼","노잼"))
txt <- gsub("ㅋ","", txt)
txt <- gsub("ㅠㅠ","",txt)
txt <- gsub("ㅜㅜ","", txt)
ts <- extractNoun(txt)
View(ts)
buildDictionary(ext_dic = "woorimalsam", category_dic_nms = "", user_dic =
data.frame(c('오태식',"핵노잼","노잼")))
buildDictionary(ext_dic = "woorimalsam", category_dic_nms = "", user_dic =
data.frame(c('오태식',"핵노잼","노잼","김래원")))
unlist(ts)
table(ts)
id_list <- character()
library(rvest)
library(stringr)
base_url <- 'https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=173123&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page='
id_list <- character()
star_list <- numeric()
txt_list <- character()
trim <- function(x) gsub("^\\s+|\\s+$", "", x)
for (i in 1:50) {
url = paste(base_url,i, sep = "")
content = read_html(url)
node1 = html_nodes(content, ".score_reple a span")
node2 = html_nodes(content, ".score_result .star_score em")
node3 = html_nodes(content, ".score_reple p")
id = html_text(node1)
star = html_text(node2)
txt = html_text(node3) %>%
trim()
id_list = append(id_list, id)
star_list = append(star_list, star)
txt_list = append(txt_list, txt)
}
df = data.frame(id_list, star_list, txt_list)
write.csv(df, file = "sm_tr.csv", row.names = FALSE)
data <- read.csv("sm_tr.csv", encoding = "euc-kr")
ds <- data %>%
select(txt_list)
write.table(ds,"st.txt", row.names = FALSE)
txt <- readLines("st.txt")
txt
for (i in 1:50) {
url = paste(base_url,i, sep = "")
content = read_html(url)
node1 = html_nodes(content, ".score_reple a span")
node2 = html_nodes(content, ".score_result .star_score em")
node3 = html_nodes(content, ".score_reple p")
id = html_text(node1)
star = html_text(node2)
txt = html_text(node3) %>%
trim() %>%
str_split("\r\n") %>%
.[[1]] -> rep
id_list = append(id_list, id)
star_list = append(star_list, star)
txt_list = append(txt_list, txt)
}
base_url <- 'https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=173123&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page='
id_list <- character()
star_list <- numeric()
txt_list <- character()
for (i in 1:50) {
url = paste(base_url,i, sep = "")
content = read_html(url)
node1 = html_nodes(content, ".score_reple a span")
node2 = html_nodes(content, ".score_result .star_score em")
node3 = html_nodes(content, ".score_reple p")
id = html_text(node1)
star = html_text(node2)
txt = html_text(node3) %>%
trim() %>%
str_split("\r\n") %>%
.[[1]] -> rep
id_list = append(id_list, id)
star_list = append(star_list, star)
txt_list = append(txt_list, txt)
}
trim <- function(x) gsub("^\\s+|\\s+$", "", x)
for (i in 1:50) {
url = paste(base_url,i, sep = "")
content = read_html(url)
node1 = html_nodes(content, ".score_reple a span")
node2 = html_nodes(content, ".score_result .star_score em")
node3 = html_nodes(content, ".score_reple p")
id = html_text(node1)
star = html_text(node2)
txt = html_text(node3) %>%
trim() %>%
str_split("\r\n") %>%
.[[1]] -> rep
id_list = append(id_list, id)
star_list = append(star_list, star)
txt_list = append(txt_list, txt)
}
txt_list
df = data.frame(id_list, star_list, txt_list)
write.csv(df, file = "sm_tr1.csv", row.names = FALSE)
data <- read.csv("sm_tr1.csv", encoding = "euc-kr")
ds <- data %>%
select(txt_list)
write.table(ds,"st1.txt", row.names = FALSE)
txt <- readLines("st1.txt")
txt <- gsub("ㅋ","", txt)
txt <- gsub("ㅠㅠ","",txt)
txt <- gsub("ㅜㅜ","", txt)
ts <- extractNoun(txt)
table(ts)
View(ts)
install.packages("xlsx")
# NAVER 영화('알라딘') 네티즌 리뷰 크롤링
library(rvest)
library(stringr)
library(dplyr)
library(xlsx)
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
url_base <- 'https://movie.naver.com'
start_url <- '/movie/bi/mi/point.nhn?code=163788'
url <- paste0(url_base, start_url)
html <- read_html(url)
html %>%
html_node('iframe.ifr') %>%
html_attr('src') -> if_url
if_url
ifr_url <- paste0(url_base, if_url)
html2 <- read_html(ifr_url)
# 총 데이터 건수로부터 총 페이지수 구하기
html2 %>%
html_node('div.score_total') %>%
html_nodes('em') -> ems
ems
